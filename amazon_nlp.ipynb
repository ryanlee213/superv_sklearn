{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import scipy\n",
    "import re\n",
    "import string\n",
    "from collections import Counter\n",
    "\n",
    "import gzip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Ryan\\Anaconda3\\envs\\tensorflow-gpu\\lib\\site-packages\\sklearn\\ensemble\\weight_boosting.py:29: DeprecationWarning: numpy.core.umath_tests is an internal NumPy module and should not be imported. It will be removed in a future NumPy release.\n",
      "  from numpy.core.umath_tests import inner1d\n"
     ]
    }
   ],
   "source": [
    "from sklearn.base import clone\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.feature_extraction.text import HashingVectorizer, TfidfVectorizer\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "from sklearn.naive_bayes import MultinomialNB, BernoulliNB\n",
    "from sklearn.linear_model import RidgeClassifier, LogisticRegression, SGDClassifier\n",
    "from sklearn import metrics\n",
    "\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "\n",
    "from nltk import word_tokenize          \n",
    "from nltk import pos_tag\n",
    "# from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import sys \n",
    "# import tensorflow as tf\n",
    "# from tensorflow import keras\n",
    "# print ('Is GPU available ? ', tf.test.is_gpu_available())\n",
    "\n",
    "\n",
    "# print(sys.version)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- reviewerID - ID of the reviewer, e.g. A2SUAM1J3GNN3B\n",
    "- asin - ID of the product, e.g. 0000013714\n",
    "- reviewerName - name of the reviewer\n",
    "- helpful - helpfulness rating of the review, e.g. 2/3\n",
    "- reviewText - text of the review\n",
    "- overall - rating of the product\n",
    "- summary - summary of the review\n",
    "- unixReviewTime - time of the review (unix time)\n",
    "- reviewTime - time of the review (raw)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *\"just based on text\"*\n",
    "- does that mean just use 'reviewText'? or including 'summary' as well?\n",
    "    - 'reviewerID'; how to check bias, scoring standards (, and maybe fake/multiple accounts?)\n",
    "    - 'reviewerName; ???\n",
    "    - 'asin'/product; group by products, price (from another file, not public)\n",
    "    - 'helpful'; check 'helpful' and 'overall' (target) relationship\n",
    "        - 'helpful' score will be lower for lower than the 'asin's average 'overall' scores\n",
    "        \n",
    "#### Will just be doing 'reviewText' and 'summary', see which translates to a better score predictor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse(path):\n",
    "  g = gzip.open(path, 'rb')\n",
    "  for l in g:\n",
    "    yield eval(l)\n",
    "\n",
    "def getDF(path, size=False):\n",
    "  i = 0\n",
    "  df = {}\n",
    "  for d in parse(path):\n",
    "    df[i] = d\n",
    "    i += 1\n",
    "    if i == 500:\n",
    "        if size==True:\n",
    "            break\n",
    "        \n",
    "    \n",
    "  return pd.DataFrame.from_dict(df, orient='index')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def undersample(X, y, undersampler):\n",
    "    \n",
    "    # undersampling using index for X\n",
    "    X_index = pd.DataFrame(X.index.values)\n",
    "    \n",
    "    X_res, y_res = undersampler.fit_sample(X_index, y)\n",
    "    \n",
    "    \n",
    "    # creating X using index(=X_res)\n",
    "    res_ind = [ind for line in X_res for ind in line]\n",
    "\n",
    "    X_res1 = X.loc[res_ind, :]\n",
    "    \n",
    "    \n",
    "    return X_res1, y_res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_split_data():\n",
    "    \n",
    "    df = getDF('reviews_Movies_and_TV_5.json.gz')\n",
    "    \n",
    "    y = df.overall\n",
    "    X = df[['reviewText', 'summary']]\n",
    "\n",
    "    # taking out punctuations\n",
    "    X.loc[:, 'reviewText'] = X.reviewText.apply(lambda x: re.sub(r'[^\\w\\s]','', x))\n",
    "    X.loc[:, 'summary'] = X.summary.apply(lambda x: re.sub(r'[^\\w\\s]','', x))\n",
    "    \n",
    "    # holdout set\n",
    "    X_df, X_holdout, y_df, y_holdout = train_test_split(X, y, test_size=0.1, stratify=y, random_state=42)\n",
    "    \n",
    "    \n",
    "    # undersampling, using index for X\n",
    "    us = RandomUnderSampler(ratio='not minority', random_state=42, )  \n",
    "\n",
    "    X_res, y_res = undersample(X_df, y_df, us)\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    # train and test set\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X_res, y_res, test_size=.3, stratify=y_res, random_state=42)\n",
    "    \n",
    "    y_bin_train = np.array(list(map(lambda x: 1 if x>=4 else 0, y_train)))\n",
    "    y_bin_test = np.array(list(map(lambda x: 1 if x>=4 else 0, y_test)))\n",
    "    y_bin_holdout = np.array(list(map(lambda x: 1 if x>=4 else 0, y_holdout)))\n",
    "    \n",
    "    \n",
    "    X_dict = {'train':X_train, 'test':X_test, 'holdout':X_holdout}\n",
    "    \n",
    "    y_dict = {'train':y_train, 'test':y_test, 'holdout':y_holdout,\n",
    "              'bin_train':y_bin_train, 'bin_test':y_bin_test, 'bin_holdout':y_bin_holdout}\n",
    "    \n",
    "    return X_dict, y_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "hash_mnb = Pipeline([('count', HashingVectorizer(stop_words='english', alternate_sign=False, n_features=2**18)), \n",
    "                     ('mnb', MultinomialNB())])\n",
    "\n",
    "hash_bnb = Pipeline([('binary', HashingVectorizer(stop_words='english', alternate_sign=False, n_features=2**18, binary=True)), \n",
    "                     ('bnb', BernoulliNB())])\n",
    "\n",
    "hash_lr = Pipeline([('count', HashingVectorizer(stop_words='english', alternate_sign=False, n_features=2**18)), \n",
    "                     ('lr', LogisticRegression(penalty='l2', solver='sag', multi_class='multinomial',\n",
    "                                                random_state=42, tol=0.0002))])\n",
    "\n",
    "\n",
    "tfidf_mnb = Pipeline([('tfidf', TfidfVectorizer(stop_words='english', sublinear_tf=True)),\n",
    "                       ('tfidf_mnb', MultinomialNB())])\n",
    "\n",
    "tfidf_lr = Pipeline([('tfidf', TfidfVectorizer(stop_words='english', sublinear_tf=True)),\n",
    "                       ('tfidf_lr', MultinomialNB())])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Bunch(object):\n",
    "  def __init__(self, adict):\n",
    "    self.__dict__.update(adict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def models_basic(models, X_dict, y_dict, i=1, review='reviewText'):\n",
    "    \n",
    "    X = Bunch(X_dict)\n",
    "    y = Bunch(y_dict)\n",
    "    \n",
    "    for model in models:\n",
    "        \n",
    "        print('----'*4, model.steps[i][0].upper(), '----'*4)\n",
    "        model2 = clone(model)\n",
    "        model.fit(X.train[review], y.train)\n",
    "\n",
    "        print('\\n')\n",
    "        print('----'*9)\n",
    "        print('Test set')\n",
    "\n",
    "        y_ = model.predict(X.test[review])\n",
    "        y_bin_ = np.array(list(map(lambda x: 1 if x>=4 else 0, y_)))\n",
    "\n",
    "        print('Raw prediction score: ', model.score(X.test[review], y.test))\n",
    "        print('Raw-to-good/bad score: ', np.mean(y_bin_ == y.bin_test) )\n",
    "\n",
    "        model2.fit(X.train[review], y.bin_train)\n",
    "\n",
    "        print('Good/bad prediction score: ', model2.score(X.test[review], y.bin_test))\n",
    "\n",
    "        print('\\n')\n",
    "        print('----'*9)\n",
    "        print('Holdout set')\n",
    "\n",
    "        y_h_ = model.predict(X.holdout[review])\n",
    "        y_h_bin_ = np.array(list(map(lambda x: 1 if x>=4 else 0, y_h_)))\n",
    "\n",
    "        print('Raw prediction score: ', model.score(X.holdout[review], y.holdout))\n",
    "        print('Raw-to-good/bad score: ', np.mean(y_h_bin_ == y.bin_holdout) )\n",
    "        print('Good/bad prediction score: ', model2.score(X.holdout[review], y.bin_holdout))\n",
    "        print('\\n\\n')\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = [hash_mnb, hash_bnb, hash_lr, tfidf_mnb, tfidf_lr]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Ryan\\Anaconda3\\envs\\tensorflow-gpu\\lib\\site-packages\\pandas\\core\\indexing.py:630: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  self.obj[item_labels[indexer[info_axis]]] = value\n"
     ]
    }
   ],
   "source": [
    "X_dict, y_dict = get_split_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------- MNB ----------------\n",
      "\n",
      "\n",
      "------------------------------------\n",
      "Test set\n",
      "Raw prediction score:  0.46835534595744066\n",
      "Raw-to-good/bad score:  0.7974886802551825\n",
      "Good/bad prediction score:  0.7169123497330999\n",
      "\n",
      "\n",
      "------------------------------------\n",
      "Holdout set\n",
      "Raw prediction score:  0.4646370630441698\n",
      "Raw-to-good/bad score:  0.7783910835679866\n",
      "Good/bad prediction score:  0.522349988807333\n",
      "\n",
      "\n",
      "\n",
      "---------------- BNB ----------------\n",
      "\n",
      "\n",
      "------------------------------------\n",
      "Test set\n",
      "Raw prediction score:  0.39037568533279327\n",
      "Raw-to-good/bad score:  0.7193281930360061\n",
      "Good/bad prediction score:  0.7023738915329756\n",
      "\n",
      "\n",
      "------------------------------------\n",
      "Holdout set\n",
      "Raw prediction score:  0.5154399896320558\n",
      "Raw-to-good/bad score:  0.8023846271663702\n",
      "Good/bad prediction score:  0.7850477750156108\n",
      "\n",
      "\n",
      "\n",
      "---------------- LR ----------------\n",
      "\n",
      "\n",
      "------------------------------------\n",
      "Test set\n",
      "Raw prediction score:  0.5094463812981903\n",
      "Raw-to-good/bad score:  0.8137486076352222\n",
      "Good/bad prediction score:  0.8205766198446338\n",
      "\n",
      "\n",
      "------------------------------------\n",
      "Holdout set\n",
      "Raw prediction score:  0.5592798991481791\n",
      "Raw-to-good/bad score:  0.8204814025000884\n",
      "Good/bad prediction score:  0.8072504918882618\n",
      "\n",
      "\n",
      "\n",
      "---------------- MNB ----------------\n",
      "\n",
      "\n",
      "------------------------------------\n",
      "Test set\n",
      "Raw prediction score:  0.4434663734864814\n",
      "Raw-to-good/bad score:  0.7923749041619049\n",
      "Good/bad prediction score:  0.648523731682266\n",
      "\n",
      "\n",
      "------------------------------------\n",
      "Holdout set\n",
      "Raw prediction score:  0.4056045807462563\n",
      "Raw-to-good/bad score:  0.755369534738504\n",
      "Good/bad prediction score:  0.3576705114459748\n",
      "\n",
      "\n",
      "\n",
      "---------------- LR ----------------\n",
      "\n",
      "\n",
      "------------------------------------\n",
      "Test set\n",
      "Raw prediction score:  0.4434663734864814\n",
      "Raw-to-good/bad score:  0.7923749041619049\n",
      "Good/bad prediction score:  0.648523731682266\n",
      "\n",
      "\n",
      "------------------------------------\n",
      "Holdout set\n",
      "Raw prediction score:  0.4056045807462563\n",
      "Raw-to-good/bad score:  0.755369534738504\n",
      "Good/bad prediction score:  0.3576705114459748\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "models_basic(models, X_dict, y_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = getDF('reviews_Movies_and_TV_5.json.gz', size=True)\n",
    "# X = df[['summary']]\n",
    "# y = df.overall\n",
    "# us = RandomUnderSampler(ratio='not minority', random_state=42, )  \n",
    "\n",
    "# X_res, y_res = us.fit_sample(X, y)\n",
    "# tfidf_mnb.fit(X['summary'], y)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
